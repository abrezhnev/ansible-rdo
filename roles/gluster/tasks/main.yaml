#--------------------
# Gluster Packages 
#  and daemons
#--------------------     

- name: ensure we have GlusterFS repos available
  copy: src=glusterfs-epel.repo  dest=/etc/yum.repos.d/glusterfs.repo owner=root group=root mode=0644

- name: install gluster packages
  yum: name=${item} state=installed
  environment: proxy_env
  with_items:
     - glusterfs-server
     - glusterfs-fuse

- name: restart gluster service
  action: service name=glusterd state=started

#---------------------------------
# Brick creation
#
# conditional on variable
#  make_gluster_loopback_brick == True
#
#----------------------------------

- name: setup directory for loopback gluster devices
  file: path={{ loopbacks_dir }} state=directory
  when: make_gluster_loopback_brick

- name: create a loopback file for Cinder
  shell: dd if=/dev/zero of={{ loopbacks_dir }}/cinder-block bs=1G count={{ gluster_brick_size }}
         creates={{ loopbacks_dir }}/cinder-block
  when: make_gluster_loopback_brick
  async: 3600
  poll: 5

- name: create a loopback file for a shared instance directory for nova
  shell: dd if=/dev/zero of={{ loopbacks_dir }}/instances-block bs=1G count={{ gluster_brick_size }}
         creates={{ loopbacks_dir }}/instances-block
  when: make_gluster_loopback_brick
  async: 3600
  poll: 5

- name: create a loopback file for swift
  shell: dd if=/dev/zero of={{ loopbacks_dir }}/swift-block bs=1G count={{ gluster_brick_size }}
         creates={{ loopbacks_dir }}/swift-block
  when: make_gluster_loopback_brick
  async: 3600
  poll: 5

# The following shold really be a loopback module.

- name: deattach associated loop device
  shell: losetup -d /dev/{{ item }}
  when: make_gluster_loopback_brick
  with_items: [ loop1 loop2 loop3 ]
  ignore_errors: yes

- name: create a loopback device for cinder
  shell: losetup /dev/loop1 {{ loopbacks_dir }}/cinder-block
  when: make_gluster_loopback_brick
  ignore_errors: yes

- name: create a loopback device for the instance dir
  shell: losetup /dev/loop2 {{ loopbacks_dir }}/instances-block
  when: make_gluster_loopback_brick
  ignore_errors: yes

- name: create a loopback device for the swift storage block
  shell: losetup /dev/loop3 {{ loopbacks_dir }}/swift-block
  when: make_gluster_loopback_brick
  ignore_errors: yes

#----------------------
# Brick mounting   
#---------------------

- name: install xfs packages
  yum: name=xfsprogs state=installed

- name: setup brick FS for gluster (will fail if already formatted)
  shell: mkfs.xfs -f -i size=512 /dev/{{ item }}
  with_items: [ loop1, loop2, loop3 ]
  ignore_errors: yes
  tags:
    - brick

- name: setup a mount point for the brick
  file: path={{ item }} state=directory
  with_items: 
    - ${brick_mount_cinder}
    - ${brick_mount_instances}
    - ${brick_mount_swift} 
  tags:
    - brick

- name: create mount entry for the new cinder brick, and mount
  mount: name={{ brick_mount_cinder }}    src=/dev/loop1 fstype=xfs opts=defaults passno=2 state=mounted
  tags:
    - brick

- name: create mount entry for the new instances brick, and mount
  mount: name={{ brick_mount_instances }} src=/dev/loop2 fstype=xfs opts=defaults passno=2 state=mounted
  tags:
    - brick

- name: create mount entry for the new swift brick, and mount
  mount: name={{ brick_mount_swift }}     src=/dev/loop3 fstype=xfs opts=defaults passno=2 state=mounted
  tags:
    - brick

#--------------------
# SELiux & firewall
#--------------------

- name: disable SELinux
  action: selinux policy=targeted state=permissive

#
#  Firewall for gluster on RH-like
#    http://gluster.org/community/documentation/index.php/Gluster_3.1:_Installing_GlusterFS_on_Red_Hat_Package_Manager_(RPM)_Distributions
# 
#
# EC2 Security group settings ...
#
#TCP       22        0.0.0.0/0 
#TCP 24000 - 24100  10.0.0.0/8 
#TCP      111       10.0.0.0/8  
#UDP      111       10.0.0.0/8  
#TCP 38465 - 38485  10.0.0.0/8  

# $ iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 24000:24100 -j ACCEPT 
# $ iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 111         -j ACCEPT 
# $ iptables -A INPUT -m state --state NEW -m udp -p udp --dport 111 -j ACCEPT 
# $ iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 38465:38485 -j ACCEPT
# $ service iptables save
# $ service iptables restart
# 

#- name: modify local firewall settings for gluster
#  action: copy src=gluster/files/iptables-gluster dest=/etc/sysconfig/iptables owner=root group=root mode=644 backup=yes

#- name: restart local firewall for gluster
#  action: service name=iptables state=restarted


